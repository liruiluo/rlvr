# @package _global_

# NPU-friendly variant of `grpo_moe_lora_sphere_hf.yaml`.

hydra:
  job:
    chdir: false
  searchpath:
    - file://${oc.env:VERL_REPO_ROOT}/verl/trainer/config

defaults:
  - /ppo_trainer
  - _self_
  - /task: chain_sum

reasoning_gym:
  dataset_size: 2000
  developer_prompt: DeepSeekZero

  datasets:
    chain_sum:
      weight: 0
      config:
        min_terms: 2
        max_terms: 5
        min_digits: 1
        max_digits: 2
        allow_negation: false

    base_conversion:
      weight: 0
      config:
        min_base: 2
        max_base: 16
        min_value: 0
        max_value: 1000

    gcd:
      weight: 0
      config:
        min_numbers: 2
        max_numbers: 3
        min_value: 1
        max_value: 10000

    spell_backward:
      weight: 0
      config:
        min_word_len: 3
        max_word_len: 8

data:
  tokenizer: null
  train_files: null
  val_files: null
  prompt_key: prompt
  max_prompt_length: 256
  max_response_length: 128
  train_batch_size: 32
  val_batch_size: 64
  filter_overlong_prompts: false
  truncation: error

actor_rollout_ref:
  hybrid_engine: True

  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: True
    use_remove_padding: False
    use_fused_kernels: False

    lora_rank: 16
    lora_alpha: 16
    lora_dropout: 0.0

  actor:
    strategy: fsdp
    ppo_mini_batch_size: 16
    ppo_micro_batch_size_per_gpu: 8
    use_dynamic_bsz: False
    ppo_max_token_len_per_gpu: 8192
    use_torch_compile: False
    use_kl_loss: False
    entropy_coeff: 0
    optim:
      lr: 5e-6
      lr_warmup_steps_ratio: 0.0
      total_training_steps: -1
    fsdp_config:
      dtype: bfloat16
      model_dtype: bf16
      param_offload: False
      optimizer_offload: False
      fsdp_size: -1

  rollout:
    name: hf
    mode: async
    tensor_model_parallel_size: 1
    free_cache_engine: False
    ignore_eos: False
    temperature: 0.7
    top_k: 0
    top_p: 0.95
    do_sample: True
    n: 8
    log_prob_micro_batch_size_per_gpu: 8
    agent:
      num_workers: 1
    custom:
      verl_ext:
        enabled: true
        use_moe_lora: true
        moe_num_experts: 4
        moe_top_k: 2
        sphere_feature_ratio: 0.05
        sphere_gating_ratio: 0.0
        sphere_eps: 1e-8
        sphere_mode: loss_ratio
        sphere_target: last
    val_kwargs:
      temperature: 0.0
      top_k: 0
      top_p: 1.0
      do_sample: False
      n: 1

critic:
  enable: False
  strategy: fsdp

reward_model:
  enable: False
  use_reward_loop: False

algorithm:
  adv_estimator: grpo
  use_kl_in_reward: False

trainer:
  device: npu
  project_name: rgym_verl
  experiment_name: chain_sum_single_npu_moe_lora_sphere_hf
  logger: ["console"]
  nnodes: 1
  n_gpus_per_node: 1
  total_epochs: 3
  test_freq: 20
  save_freq: -1
  val_before_train: True
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  default_hdfs_dir: null

crl:
  enabled: false
  seq: safe
  tasks: []
  steps_per_phase: 16
  replay_weight: 0.0
  eval_at_phase_start: true
  dump_eval_samples_at_phase_start: false
  save_ckpt_at_phase_end: true
  eval_at_phase_end: true
  dump_eval_samples_at_phase_end: true
  phase_task: null
  phase_index: null

ray:
  address: local
  include_dashboard: false
