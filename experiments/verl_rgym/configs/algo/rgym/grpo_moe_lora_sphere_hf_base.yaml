# @package _global_

# Sweeps: run with `-m seed=0,1,2`.
seed: 0
seed_offset: 42

reasoning_gym:
  dataset_size: 2000
  developer_prompt: DeepSeekZero

  # Keep task specs here so CRL replay-weighting can reference previous tasks
  # without having to load per-task configs.
  datasets:
    basic_arithmetic:
      weight: 0
      config:
        min_terms: 2
        max_terms: 4
        min_digits: 1
        max_digits: 1
        operators: ["+"]
        allow_parentheses: false
        allow_negation: false
        format_style: simple
        whitespace: single

    chain_sum:
      weight: 0
      config:
        min_terms: 2
        max_terms: 5
        min_digits: 1
        max_digits: 2
        allow_negation: false

    base_conversion:
      weight: 0
      config:
        min_base: 2
        max_base: 16
        min_value: 0
        max_value: 255

    gcd:
      weight: 0
      config:
        min_numbers: 2
        max_numbers: 2
        min_value: 1
        max_value: 1000

    lcm:
      weight: 0
      config:
        min_numbers: 2
        max_numbers: 2
        min_value: 1
        max_value: 100

    caesar_cipher:
      weight: 0
      config:
        delimiter: "."
        min_words: 3
        max_words: 8
        min_rotation: 1
        max_rotation: 13

    group_anagrams:
      weight: 0
      config:
        min_anagram_groups: 2
        max_anagram_groups: 4
        min_words_per_group: 2
        max_words_per_group: 3

    spell_backward:
      weight: 0
      config:
        min_word_len: 3
        max_word_len: 8

data:
  tokenizer: null
  train_files: null
  val_files: null
  prompt_key: prompt
  max_prompt_length: 256
  max_response_length: 128
  train_batch_size: 32
  val_batch_size: 64
  filter_overlong_prompts: false
  truncation: error

actor_rollout_ref:
  hybrid_engine: True

  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: True
    use_remove_padding: True

    lora_rank: 16
    lora_alpha: 16
    lora_dropout: 0.0

  actor:
    strategy: fsdp
    ppo_mini_batch_size: 16
    ppo_micro_batch_size_per_gpu: 8
    use_dynamic_bsz: False
    ppo_max_token_len_per_gpu: 8192
    use_torch_compile: False
    use_kl_loss: False
    entropy_coeff: 0
    optim:
      lr: 5e-6
      lr_warmup_steps_ratio: 0.0
      total_training_steps: -1
    fsdp_config:
      dtype: bfloat16
      model_dtype: bf16
      param_offload: False
      optimizer_offload: False
      fsdp_size: -1

  rollout:
    name: hf
    mode: async
    tensor_model_parallel_size: 1
    free_cache_engine: False
    ignore_eos: False
    temperature: 0.7
    top_k: 0
    top_p: 0.95
    do_sample: True
    n: 8
    log_prob_micro_batch_size_per_gpu: 8
    agent:
      num_workers: 1
    custom:
      verl_ext:
        enabled: true
        use_moe_lora: true
        moe_num_experts: 4
        moe_top_k: 2
        sphere_feature_ratio: 0.05
        sphere_gating_ratio: 0.0
        sphere_eps: 1e-8
        sphere_mode: loss_ratio
        sphere_target: last
        # HF-local rollout micro-batching (verl_ext/HFLocalAsyncRollout)
        hf_max_batch_size: 8
        hf_batch_wait_ms: 2.0
    val_kwargs:
      temperature: 0.0
      top_k: 0
      top_p: 1.0
      do_sample: False
      n: 1

critic:
  enable: False
  strategy: fsdp

reward_model:
  enable: False
  use_reward_loop: False

algorithm:
  adv_estimator: grpo
  use_kl_in_reward: False

trainer:
  project_name: rgym_verl
  logger: ["console"]
  resume_mode: auto
  nnodes: 1
  n_gpus_per_node: 1
  total_epochs: 999999
  total_training_steps: 500
  test_freq: 20
  save_freq: 100
  val_before_train: True
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}
  default_hdfs_dir: null

crl:
  enabled: false
  seq: safe
  tasks: []
  steps_per_phase: 500
  replay_weight: 0.0
  save_freq: 8
  eval_at_phase_start: true
  dump_eval_samples_at_phase_start: false
  save_ckpt_at_phase_end: true
  eval_at_phase_end: true
  dump_eval_samples_at_phase_end: true
  phase_task: null
  phase_index: null

ray:
  address: local
  include_dashboard: false
